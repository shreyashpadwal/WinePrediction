{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6dc9f26",
   "metadata": {},
   "source": [
    "# Model Training ‚Äî Improved\n",
    "\n",
    "This notebook demonstrates a robust training workflow:\n",
    "- Load preprocessed dataset\n",
    "- Train/test split with stratification\n",
    "- Use a pipeline combining preprocessing + model\n",
    "- Cross-validation with `cross_val_score` and `cross_validate`\n",
    "- Use `GridSearchCV` for hyperparameter tuning\n",
    "- Evaluate on hold-out test set and save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbabcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Libraries imported successfully!\n",
      "üöÄ Starting model training pipeline...\n",
      "üç∑ Loaded preprocessed wine dataset!\n",
      "üìä Dataset shape: (1359, 12)\n",
      "üìã Columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality_binary']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('üìñ Libraries imported successfully!')\n",
    "print('üöÄ Starting model training pipeline...')\n",
    "\n",
    "# Load preprocessed data\n",
    "DATA_PATH = '../data/processed/wine_processed.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('üç∑ Loaded preprocessed wine dataset!')\n",
    "    print(f'üìä Dataset shape: {df.shape}')\n",
    "    print(f'üìã Columns: {list(df.columns)}')\n",
    "except Exception as e:\n",
    "    print('Could not load dataset:', e)\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7af54",
   "metadata": {},
   "source": [
    "## 1) Prepare X, y and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d7af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Data preparation:\n",
      "‚Ä¢ Features: 11\n",
      "‚Ä¢ Feature names: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
      "‚Ä¢ Target variable: quality_binary\n",
      "‚Ä¢ Target classes: ['Good' 'Bad']\n",
      "‚Ä¢ Encoded target classes: ['Bad' 'Good']\n",
      "‚Ä¢ Encoded values: [0 1]\n",
      "üìö Train shape: (1087, 11), Test shape: (272, 11)\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # Prepare features and target\n",
    "    features = [col for col in df.columns if col != 'quality_binary']\n",
    "    X = df[features]\n",
    "    y = df['quality_binary']\n",
    "\n",
    "    # Initialize label encoder\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    print(f'üîß Data preparation:')\n",
    "    print(f'‚Ä¢ Features: {len(features)}')\n",
    "    print(f'‚Ä¢ Feature names: {features}')\n",
    "    print(f'‚Ä¢ Target variable: quality_binary')\n",
    "    print(f'‚Ä¢ Target classes: {y.unique()}')\n",
    "    print(f'‚Ä¢ Encoded target classes: {le.classes_}')\n",
    "    print(f'‚Ä¢ Encoded values: {np.unique(y_encoded)}')\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "    print(f'üìö Train shape: {X_train.shape}, Test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0818fcf",
   "metadata": {},
   "source": [
    "## 2) Build pipeline (preprocessing + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35aba998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline not found; it will be built from training data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load preprocessing pipeline saved earlier if available\n",
    "try:\n",
    "    preproc = joblib.load(PIPE_PATH)\n",
    "    print('Loaded preprocessing pipeline from', PIPE_PATH)\n",
    "except:\n",
    "    preproc = None\n",
    "    print('Preprocessing pipeline not found; it will be built from training data.')\n",
    "\n",
    "if preproc is None and df is not None:\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    # Re-build minimal preproc using training data\n",
    "    num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "    cat_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "    num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "    cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='__MISSING__')), ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))])\n",
    "    preproc = ColumnTransformer([('num', num_pipeline, num_cols), ('cat', cat_pipeline, cat_cols)], remainder='drop')\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "pipe = Pipeline([('preproc', preproc), ('clf', model)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aabde2",
   "metadata": {},
   "source": [
    "## 3) Cross-validation (stratified) and baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89924680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time 0.3645845890045166\n",
      "score_time 0.06795239448547363\n",
      "test_accuracy 0.9503234262038642\n",
      "test_f1_weighted 0.9350862673099749\n",
      "test_roc_auc 0.8285955499814195\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = ['accuracy', 'f1_weighted', 'roc_auc'] if len(np.unique(y_train))==2 else ['accuracy','f1_weighted']\n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "    for k,v in scores.items():\n",
    "        print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc7382",
   "metadata": {},
   "source": [
    "## 4) Hyperparameter tuning (GridSearchCV) ‚Äî example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "492aef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Training: Logistic Regression\n",
      "‚úÖ Logistic Regression - Accuracy: 0.9485, F1: 0.9736\n",
      "\n",
      "üîπ Training: Decision Tree\n",
      "‚úÖ Decision Tree - Accuracy: 0.9228, F1: 0.9597\n",
      "\n",
      "üîπ Training: Random Forest\n",
      "‚úÖ Random Forest - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "üîπ Training: Gradient Boosting\n",
      "‚úÖ Gradient Boosting - Accuracy: 0.9375, F1: 0.9676\n",
      "\n",
      "üîπ Training: SVM\n",
      "‚úÖ Random Forest - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "üîπ Training: Gradient Boosting\n",
      "‚úÖ Gradient Boosting - Accuracy: 0.9375, F1: 0.9676\n",
      "\n",
      "üîπ Training: SVM\n",
      "‚úÖ SVM - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "üîπ Training: XGBoost\n",
      "‚úÖ XGBoost - Accuracy: 0.9632, F1: 0.9811\n",
      "\n",
      "üìä Model Performance Summary:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.948529   0.952030  0.996139  0.973585\n",
      "1        Decision Tree  0.922794   0.954198  0.965251  0.959693\n",
      "2        Random Forest  0.952206   0.952206  1.000000  0.975518\n",
      "3    Gradient Boosting  0.937500   0.954887  0.980695  0.967619\n",
      "4                  SVM  0.952206   0.952206  1.000000  0.975518\n",
      "5              XGBoost  0.963235   0.962825  1.000000  0.981061\n",
      "\n",
      "üèÜ Best Model: XGBoost\n",
      "‚úÖ SVM - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "üîπ Training: XGBoost\n",
      "‚úÖ XGBoost - Accuracy: 0.9632, F1: 0.9811\n",
      "\n",
      "üìä Model Performance Summary:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.948529   0.952030  0.996139  0.973585\n",
      "1        Decision Tree  0.922794   0.954198  0.965251  0.959693\n",
      "2        Random Forest  0.952206   0.952206  1.000000  0.975518\n",
      "3    Gradient Boosting  0.937500   0.954887  0.980695  0.967619\n",
      "4                  SVM  0.952206   0.952206  1.000000  0.975518\n",
      "5              XGBoost  0.963235   0.962825  1.000000  0.981061\n",
      "\n",
      "üèÜ Best Model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f'\\nüîπ Training: {name}')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "        print(f'‚úÖ {name} - Accuracy: {acc:.4f}, F1: {f1:.4f}')\n",
    "\n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print('\\nüìä Model Performance Summary:')\n",
    "    print(results_df)\n",
    "\n",
    "    # Select best model based on F1 Score\n",
    "    best_model_name = results_df.loc[results_df['F1 Score'].idxmax(), 'Model']\n",
    "    best_model = models[best_model_name]\n",
    "    print(f'\\nüèÜ Best Model: {best_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7e698",
   "metadata": {},
   "source": [
    "## 5) Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82b29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9632352941176471\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.23      0.38        13\n",
      "           1       0.96      1.00      0.98       259\n",
      "\n",
      "    accuracy                           0.96       272\n",
      "   macro avg       0.98      0.62      0.68       272\n",
      "weighted avg       0.96      0.96      0.95       272\n",
      "\n",
      "ROC AUC: 0.7986337986337987\n",
      "Confusion matrix:\n",
      " [[  3  10]\n",
      " [  0 259]]\n",
      "Saved best model to ../backend/saved_models/best_model.joblib\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    try:\n",
    "        if len(np.unique(y_test))==2:\n",
    "            print('ROC AUC:', roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))\n",
    "    except Exception as e:\n",
    "        print('ROC AUC error:', e)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "    # save model to project folder so path exists on Windows/local\n",
    "    import os\n",
    "    os.makedirs('../backend/saved_models', exist_ok=True)\n",
    "    joblib.dump(best_model, '../backend/saved_models/best_model.joblib')\n",
    "    print('Saved best model to ../backend/saved_models/best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "416a4b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Best model saved to ../backend/saved_models/best_model.pkl\n",
      "üíæ Label encoder saved to ../backend/saved_models/label_encoder.pkl\n",
      "üíæ Model metrics saved to ../backend/saved_models/model_metrics.json\n",
      "\n",
      "üéâ Model training pipeline completed successfully!\n",
      "Best performing model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../backend/saved_models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "if 'best_model' in globals():\n",
    "    # Save model\n",
    "    joblib.dump(best_model, '../backend/saved_models/best_model.pkl')\n",
    "    print('üíæ Best model saved to ../backend/saved_models/best_model.pkl')\n",
    "\n",
    "    # Save label encoder\n",
    "    if 'le' in globals():\n",
    "        joblib.dump(le.classes_, '../backend/saved_models/label_encoder.pkl')\n",
    "        print('üíæ Label encoder saved to ../backend/saved_models/label_encoder.pkl')\n",
    "\n",
    "    # Save model metrics\n",
    "    if 'results_df' in globals():\n",
    "        metrics_dict = results_df.set_index('Model').to_dict(orient='index')\n",
    "        with open('../backend/saved_models/model_metrics.json', 'w') as f:\n",
    "            json.dump(metrics_dict, f, indent=4)\n",
    "        print('üíæ Model metrics saved to ../backend/saved_models/model_metrics.json')\n",
    "\n",
    "    print('\\nüéâ Model training pipeline completed successfully!')\n",
    "    print(f'Best performing model: {best_model_name}')\n",
    "else:\n",
    "    print('ERROR: best_model is not defined. Please run the training cells first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe08b87d",
   "metadata": {},
   "source": [
    "## Notes & next steps\n",
    "- If classes are imbalanced, consider `class_weight='balanced'` in tree models or use oversampling (SMOTE) inside a pipeline.\n",
    "- For high-cardinality categoricals, consider Target Encoding (use `category_encoders` library) or embedding approaches.\n",
    "- Try XGBoost/LightGBM if available ‚Äî they often improve performance.\n",
    "- Feature selection (SHAP, permutation importance) can further improve results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
