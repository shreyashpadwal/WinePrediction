{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6dc9f26",
   "metadata": {},
   "source": [
    "# Model Training — Improved\n",
    "\n",
    "This notebook demonstrates a robust training workflow:\n",
    "- Load preprocessed dataset\n",
    "- Train/test split with stratification\n",
    "- Use a pipeline combining preprocessing + model\n",
    "- Cross-validation with `cross_val_score` and `cross_validate`\n",
    "- Use `GridSearchCV` for hyperparameter tuning\n",
    "- Evaluate on hold-out test set and save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbabcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 Libraries imported successfully!\n",
      "🚀 Starting model training pipeline...\n",
      "🍷 Loaded preprocessed wine dataset!\n",
      "📊 Dataset shape: (1359, 12)\n",
      "📋 Columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality_binary']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('📖 Libraries imported successfully!')\n",
    "print('🚀 Starting model training pipeline...')\n",
    "\n",
    "# Load preprocessed data\n",
    "DATA_PATH = '../data/processed/wine_processed.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print('🍷 Loaded preprocessed wine dataset!')\n",
    "    print(f'📊 Dataset shape: {df.shape}')\n",
    "    print(f'📋 Columns: {list(df.columns)}')\n",
    "except Exception as e:\n",
    "    print('Could not load dataset:', e)\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7af54",
   "metadata": {},
   "source": [
    "## 1) Prepare X, y and train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d7af4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Data preparation:\n",
      "• Features: 11\n",
      "• Feature names: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
      "• Target variable: quality_binary\n",
      "• Target classes: ['Good' 'Bad']\n",
      "• Encoded target classes: ['Bad' 'Good']\n",
      "• Encoded values: [0 1]\n",
      "📚 Train shape: (1087, 11), Test shape: (272, 11)\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # Prepare features and target\n",
    "    features = [col for col in df.columns if col != 'quality_binary']\n",
    "    X = df[features]\n",
    "    y = df['quality_binary']\n",
    "\n",
    "    # Initialize label encoder\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    print(f'🔧 Data preparation:')\n",
    "    print(f'• Features: {len(features)}')\n",
    "    print(f'• Feature names: {features}')\n",
    "    print(f'• Target variable: quality_binary')\n",
    "    print(f'• Target classes: {y.unique()}')\n",
    "    print(f'• Encoded target classes: {le.classes_}')\n",
    "    print(f'• Encoded values: {np.unique(y_encoded)}')\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "    print(f'📚 Train shape: {X_train.shape}, Test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0818fcf",
   "metadata": {},
   "source": [
    "## 2) Build pipeline (preprocessing + model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35aba998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline not found; it will be built from training data.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load preprocessing pipeline saved earlier if available\n",
    "try:\n",
    "    preproc = joblib.load(PIPE_PATH)\n",
    "    print('Loaded preprocessing pipeline from', PIPE_PATH)\n",
    "except:\n",
    "    preproc = None\n",
    "    print('Preprocessing pipeline not found; it will be built from training data.')\n",
    "\n",
    "if preproc is None and df is not None:\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    # Re-build minimal preproc using training data\n",
    "    num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "    cat_cols = X_train.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "    num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "    cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='__MISSING__')), ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))])\n",
    "    preproc = ColumnTransformer([('num', num_pipeline, num_cols), ('cat', cat_pipeline, cat_cols)], remainder='drop')\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n",
    "pipe = Pipeline([('preproc', preproc), ('clf', model)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aabde2",
   "metadata": {},
   "source": [
    "## 3) Cross-validation (stratified) and baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89924680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time 0.3645845890045166\n",
      "score_time 0.06795239448547363\n",
      "test_accuracy 0.9503234262038642\n",
      "test_f1_weighted 0.9350862673099749\n",
      "test_roc_auc 0.8285955499814195\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scoring = ['accuracy', 'f1_weighted', 'roc_auc'] if len(np.unique(y_train))==2 else ['accuracy','f1_weighted']\n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "    for k,v in scores.items():\n",
    "        print(k, np.mean(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc7382",
   "metadata": {},
   "source": [
    "## 4) Hyperparameter tuning (GridSearchCV) — example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "492aef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training: Logistic Regression\n",
      "✅ Logistic Regression - Accuracy: 0.9485, F1: 0.9736\n",
      "\n",
      "🔹 Training: Decision Tree\n",
      "✅ Decision Tree - Accuracy: 0.9228, F1: 0.9597\n",
      "\n",
      "🔹 Training: Random Forest\n",
      "✅ Random Forest - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "🔹 Training: Gradient Boosting\n",
      "✅ Gradient Boosting - Accuracy: 0.9375, F1: 0.9676\n",
      "\n",
      "🔹 Training: SVM\n",
      "✅ Random Forest - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "🔹 Training: Gradient Boosting\n",
      "✅ Gradient Boosting - Accuracy: 0.9375, F1: 0.9676\n",
      "\n",
      "🔹 Training: SVM\n",
      "✅ SVM - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "🔹 Training: XGBoost\n",
      "✅ XGBoost - Accuracy: 0.9632, F1: 0.9811\n",
      "\n",
      "📊 Model Performance Summary:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.948529   0.952030  0.996139  0.973585\n",
      "1        Decision Tree  0.922794   0.954198  0.965251  0.959693\n",
      "2        Random Forest  0.952206   0.952206  1.000000  0.975518\n",
      "3    Gradient Boosting  0.937500   0.954887  0.980695  0.967619\n",
      "4                  SVM  0.952206   0.952206  1.000000  0.975518\n",
      "5              XGBoost  0.963235   0.962825  1.000000  0.981061\n",
      "\n",
      "🏆 Best Model: XGBoost\n",
      "✅ SVM - Accuracy: 0.9522, F1: 0.9755\n",
      "\n",
      "🔹 Training: XGBoost\n",
      "✅ XGBoost - Accuracy: 0.9632, F1: 0.9811\n",
      "\n",
      "📊 Model Performance Summary:\n",
      "                 Model  Accuracy  Precision    Recall  F1 Score\n",
      "0  Logistic Regression  0.948529   0.952030  0.996139  0.973585\n",
      "1        Decision Tree  0.922794   0.954198  0.965251  0.959693\n",
      "2        Random Forest  0.952206   0.952206  1.000000  0.975518\n",
      "3    Gradient Boosting  0.937500   0.954887  0.980695  0.967619\n",
      "4                  SVM  0.952206   0.952206  1.000000  0.975518\n",
      "5              XGBoost  0.963235   0.962825  1.000000  0.981061\n",
      "\n",
      "🏆 Best Model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        print(f'\\n🔹 Training: {name}')\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1 Score': f1\n",
    "        })\n",
    "        print(f'✅ {name} - Accuracy: {acc:.4f}, F1: {f1:.4f}')\n",
    "\n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print('\\n📊 Model Performance Summary:')\n",
    "    print(results_df)\n",
    "\n",
    "    # Select best model based on F1 Score\n",
    "    best_model_name = results_df.loc[results_df['F1 Score'].idxmax(), 'Model']\n",
    "    best_model = models[best_model_name]\n",
    "    print(f'\\n🏆 Best Model: {best_model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a7e698",
   "metadata": {},
   "source": [
    "## 5) Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e82b29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9632352941176471\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.23      0.38        13\n",
      "           1       0.96      1.00      0.98       259\n",
      "\n",
      "    accuracy                           0.96       272\n",
      "   macro avg       0.98      0.62      0.68       272\n",
      "weighted avg       0.96      0.96      0.95       272\n",
      "\n",
      "ROC AUC: 0.7986337986337987\n",
      "Confusion matrix:\n",
      " [[  3  10]\n",
      " [  0 259]]\n",
      "Saved best model to ../backend/saved_models/best_model.joblib\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    try:\n",
    "        if len(np.unique(y_test))==2:\n",
    "            print('ROC AUC:', roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))\n",
    "    except Exception as e:\n",
    "        print('ROC AUC error:', e)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "    # save model to project folder so path exists on Windows/local\n",
    "    import os\n",
    "    os.makedirs('../backend/saved_models', exist_ok=True)\n",
    "    joblib.dump(best_model, '../backend/saved_models/best_model.joblib')\n",
    "    print('Saved best model to ../backend/saved_models/best_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "416a4b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Best model saved to ../backend/saved_models/best_model.pkl\n",
      "💾 Label encoder saved to ../backend/saved_models/label_encoder.pkl\n",
      "💾 Model metrics saved to ../backend/saved_models/model_metrics.json\n",
      "\n",
      "🎉 Model training pipeline completed successfully!\n",
      "Best performing model: XGBoost\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../backend/saved_models', exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "if 'best_model' in globals():\n",
    "    # Save model\n",
    "    joblib.dump(best_model, '../backend/saved_models/best_model.pkl')\n",
    "    print('💾 Best model saved to ../backend/saved_models/best_model.pkl')\n",
    "\n",
    "    # Save label encoder\n",
    "    if 'le' in globals():\n",
    "        joblib.dump(le.classes_, '../backend/saved_models/label_encoder.pkl')\n",
    "        print('💾 Label encoder saved to ../backend/saved_models/label_encoder.pkl')\n",
    "\n",
    "    # Save model metrics\n",
    "    if 'results_df' in globals():\n",
    "        metrics_dict = results_df.set_index('Model').to_dict(orient='index')\n",
    "        with open('../backend/saved_models/model_metrics.json', 'w') as f:\n",
    "            json.dump(metrics_dict, f, indent=4)\n",
    "        print('💾 Model metrics saved to ../backend/saved_models/model_metrics.json')\n",
    "\n",
    "    print('\\n🎉 Model training pipeline completed successfully!')\n",
    "    print(f'Best performing model: {best_model_name}')\n",
    "else:\n",
    "    print('ERROR: best_model is not defined. Please run the training cells first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe08b87d",
   "metadata": {},
   "source": [
    "## Notes & next steps\n",
    "- If classes are imbalanced, consider `class_weight='balanced'` in tree models or use oversampling (SMOTE) inside a pipeline.\n",
    "- For high-cardinality categoricals, consider Target Encoding (use `category_encoders` library) or embedding approaches.\n",
    "- Try XGBoost/LightGBM if available — they often improve performance.\n",
    "- Feature selection (SHAP, permutation importance) can further improve results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
